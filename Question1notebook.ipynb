{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "class MyNeuralNetwork():\n",
    "    \"\"\"\n",
    "    My implementation of a Neural Network Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
    "    weight_inits = ['zero', 'random', 'normal']\n",
    "    \n",
    "\n",
    "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
    "      \n",
    "\n",
    "        \"\"\"\n",
    "        Initializing a new MyNeuralNetwork object\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers : int value specifying the number of layers\n",
    "\n",
    "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
    "\n",
    "        activation : string specifying the activation function to be used\n",
    "                     possible inputs: relu, sigmoid, linear, tanh\n",
    "\n",
    "        learning_rate : float value specifying the learning rate to be used\n",
    "\n",
    "        weight_init : string specifying the weight initialization function to be used\n",
    "                      possible inputs: zero, random, normal\n",
    "\n",
    "        batch_size : int value specifying the batch size to be used\n",
    "\n",
    "        num_epochs : int value specifying the number of epochs to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if activation not in self.acti_fns:\n",
    "            raise Exception('Incorrect Activation Function')\n",
    "\n",
    "        if weight_init not in self.weight_inits:\n",
    "            raise Exception('Incorrect Weight Initialization Function')\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_sizes=layer_sizes\n",
    "        self.activation=activation\n",
    "        self.learning_rate=learning_rate\n",
    "        self.weight_init=weight_init\n",
    "        self.batch_size=batch_size\n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "\n",
    "        pass\n",
    "\n",
    "    def relu(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return  X * (X>=0)\n",
    "        \n",
    "\n",
    "    def relu_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return 1*(X>=0)\n",
    "\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-X))\n",
    "\n",
    "    def sigmoid_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Sigmoid activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        sgd=self.sigmoid(X)\n",
    "        return  sgd*(1-sgd)\n",
    "       \n",
    "\n",
    "    def linear(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def linear_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Linear activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return np.ones(X.shape)\n",
    "\n",
    "    def tanh(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def tanh_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Tanh activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return 1-np.tanh(X)*np.tanh(X)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.exp(X)/(np.sum(np.exp(X),axis = 1, keepdims = True))\n",
    "\n",
    "    def softmax_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Softmax activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def zero_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Zero Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    def random_init(self, shape):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Random Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        return np.random.rand(shape[0], shape[1])*0.01\n",
    "\n",
    "    def normal_init(self, shape):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        k=np.random.normal(size = shape, scale = 0.01)\n",
    "        return k\n",
    "        \n",
    "    def one_hot_encoding(self,y):\n",
    "        y1 = np.zeros((len(y),np.max(y)+1))\n",
    "        for i in range(0,len(y)):\n",
    "            y1[i,y[i]] = 1\n",
    "        return y1\n",
    "        \n",
    "    \n",
    "    def fit_batch_grad(self,X,y,x_test=None,y_test=None):\n",
    "        \n",
    "        \n",
    "        m , n_0 = X.shape\n",
    "        n_l = y.shape[1]\n",
    "\n",
    "        weights,biases = self.initialize_func()\n",
    "        self.weights = weights\n",
    "        self.biases=biases\n",
    "\n",
    "        train_loss_history = []\n",
    "        \n",
    "        test_loss_history = []\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs), desc = \"Progress Total : \", position = 0, leave = True):\n",
    "\n",
    "\n",
    "            n_batch = m//self.batch_size\n",
    "            X_batch = [X[self.batch_size*i:self.batch_size*(i+1),:] for i in range(0,n_batch)]\n",
    "            y_batch = [y[self.batch_size*i:self.batch_size*(i+1),:] for i in range(0,n_batch)]\n",
    "\n",
    "            batch_loss_train = []\n",
    "            batch_loss_test = []\n",
    "            \n",
    "\n",
    "            for currx, curry in tqdm(zip(X_batch,y_batch), desc = \"Progress Epoch: \" + str(epoch+1) + \"/\" + str(self.num_epochs), position = 0, leave = True, total = len(X_batch)):\n",
    "                A, activation, preactivation = self.forward(currx,weights,biases)\n",
    "                \n",
    "                \n",
    "                batch_loss_train.append(self.cross_entropy(A,curry))\n",
    "\n",
    "                self.backward(currx,curry, activation,preactivation )\n",
    "\n",
    "                if(x_test is not None):\n",
    "                    proba = self.predict_proba(x_test)\n",
    "                   \n",
    "                    testloss = self.cross_entropy(proba, self.one_hot_encoding(y_test))\n",
    "                    batch_loss_test.append(testloss)\n",
    "\n",
    "            print(\"Validation loss = \" ,np.array(batch_loss_test).mean())\n",
    "            print(\"Training Loss = \", np.array(batch_loss_train).mean())\n",
    "            \n",
    "\n",
    "\n",
    "            train_loss_history.append( np.array(batch_loss_train).mean())\n",
    "\n",
    "            test_loss_history.append( np.array(batch_loss_test).mean())\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "        self.train_loss_history = train_loss_history\n",
    "        \n",
    "        self.test_loss_history = test_loss_history\n",
    "        \n",
    "        \n",
    "        self.weights = weights\n",
    "        self.biases=biases\n",
    "        \n",
    "\n",
    "\n",
    "        return self\n",
    "    def fit(self, X, y,x_test=None,y_test=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Fitting (training) the linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : an instance of self\n",
    "        \"\"\"\n",
    "        \n",
    "        y1 = self.one_hot_encoding(y)\n",
    "        self.fit_batch_grad(X,y1,x_test,y_test)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    def forward(self,X,weights,biases):\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        a=X\n",
    "        activation={}\n",
    "        preactivation={}\n",
    "        for i in range(len(weights)-1):\n",
    "            \n",
    "                \n",
    "              \n",
    "            z=np.dot(a,weights[str(i+1)])+biases[str(i+1)]\n",
    "            preactivation[str(i+1)]=z\n",
    "            if(self.activation == \"relu\"):\n",
    "                a = self.relu(z)\n",
    "\n",
    "            elif (self.activation == \"tanh\"):\n",
    "                a = self.tanh(z)\n",
    "\n",
    "            elif (self.activation == \"linear\"):\n",
    "                a = self.linear(z)\n",
    "\n",
    "            elif (self.activation == \"sigmoid\"):\n",
    "                a = self.sigmoid(z)\n",
    "            \n",
    "            \n",
    "            activation[str(i+1)]=a\n",
    "            \n",
    "         \n",
    "        Z_last = np.dot(a, weights[str(len(weights))]) + biases[ str(len(weights))]        \n",
    "        A_last = self.softmax(Z_last) \n",
    "        \n",
    "        \n",
    "        activation[str(len(weights))] = A_last\n",
    "        preactivation[ str(len(weights))] = Z_last\n",
    "            \n",
    "        \n",
    "        return A_last,activation,preactivation\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "    def backward(self,X,y,activation,preactivation):\n",
    "        \n",
    "        derw={}\n",
    "        derb={}\n",
    "        Xlen=len(X)\n",
    "        actlen = len(activation)\n",
    "        activation[\"0\"] = X\n",
    "        A = activation[str(actlen)]\n",
    "        dZ = A - y\n",
    "        derw[str(actlen)] = np.dot(activation[str(actlen-1) ].T, dZ)/Xlen\n",
    "        derb[str(actlen)] = np.sum(dZ, axis=0, keepdims=True) /Xlen\n",
    "        \n",
    "\n",
    "        devaprev = np.dot(dZ, self.weights[str(actlen)].T)\n",
    "\n",
    "        \n",
    "        \n",
    "        #print(actlen)\n",
    "\n",
    "        for i in range(actlen - 1, 0, -1):\n",
    "            \n",
    "            if(self.activation == \"relu\"):\n",
    "                dt = self.relu_grad(preactivation[str(i)])\n",
    "\n",
    "            elif (self.activation == \"tanh\"):\n",
    "                dt = self.tanh_grad(preactivation[str(i)])\n",
    "\n",
    "            elif (self.activation == \"linear\"):\n",
    "                dt = self.linear_grad(preactivation[str(i)])\n",
    "\n",
    "            elif (self.activation == \"sigmoid\"):\n",
    "                dt = self.sigmoid_grad(preactivation[str(i)])\n",
    "                \n",
    "            #print(i)\n",
    "            \n",
    "\n",
    "            dZ =devaprev*dt\n",
    "            dW = (1/Xlen) * np.dot(activation[str(i-1)].T, dZ)\n",
    "            db = (1/Xlen) * np.sum(dZ,keepdims=True,axis=0)\n",
    "            \n",
    "            devaprev = np.dot(dZ,self.weights[str(i)].T)\n",
    "                \n",
    "                \n",
    "            #print(\"i value\")\n",
    "            #print(i)\n",
    "            #print(type(db))\n",
    "            \n",
    "        \n",
    "            #print(type(dW))\n",
    "            \n",
    "            derw[str(int(i))]=dW\n",
    "            #print(i)\n",
    "            derb[str(int(i))]=db\n",
    "            \n",
    "    \n",
    "\n",
    "        for i in range(0,actlen):\n",
    "            self.weights[str(i+1)] = self.weights[str(i+1)] - self.learning_rate*derw[str(i+1)]\n",
    "            self.biases[str(i+1)] = self.biases[str(i+1)] - self.learning_rate*derb[str(i+1)]\n",
    "        return \n",
    "        \n",
    "      \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicting probabilities using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the \n",
    "            class wise prediction probabilities.\n",
    "        \"\"\"\n",
    "        prob,act,preact = self.forward(X,self.weights,self.biases)\n",
    "        return prob\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
    "        \"\"\" \n",
    "     \n",
    "        y_pred = np.argmax(self.predict_proba(X), axis = 1)\n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "    def initialize_func(self):\n",
    "        \n",
    "        weights={}\n",
    "        biases={}\n",
    "        layers = self.layer_sizes\n",
    "        lcount=self.n_layers\n",
    "        for i in range(0,lcount-1):\n",
    "            if(self.weight_init == 'zero'):\n",
    "                currentlayer = self.zero_init((layers[i],layers[i+1]))\n",
    "\n",
    "            elif(self.weight_init == 'random'):\n",
    "                currentlayer = self.random_init((layers[i],layers[i+1]))\n",
    "\n",
    "            elif(self.weight_init == 'normal'):\n",
    "                currentlayer = self.normal_init((layers[i],layers[i+1]))\n",
    "\n",
    "            \n",
    "            weights[str(i+1)]=currentlayer\n",
    "            biases[str(i+1)]=np.zeros((1,layers[i+1]))\n",
    "        self.weights=weights\n",
    "        self.biases=biases\n",
    "\n",
    "        \n",
    "\n",
    "        return weights,biases\n",
    "        \n",
    "\n",
    "    def cross_entropy(self,a,y):\n",
    "        \n",
    "        \n",
    "        \n",
    "        plog = - np.log(a[np.arange(len(y)), y.argmax(axis=1)])\n",
    "        \n",
    "        cost = np.sum(plog)/len(y)\n",
    "        return cost\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        acc : float value specifying the accuracy of the model on the provided testing set\n",
    "        \"\"\"\n",
    "        \n",
    "        correct=0\n",
    "        y_pred=self.predict(X)\n",
    "        for i in range(0,len(y_pred)):\n",
    "            if y[i]==y_pred[i]:\n",
    "                correct=correct+1\n",
    "        \n",
    "            \n",
    "        return correct/len(y)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('mnist_train.csv')\n",
    "test_df = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "\n",
    "dataset = train_df.to_numpy()\n",
    "testset = test_df.to_numpy()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = dataset[:, 1:]/255\n",
    "X_test = testset[:, 1:]/255\n",
    "standardscalar = StandardScaler()\n",
    "X_train = standardscalar.fit_transform(X_train)\n",
    "X_test = standardscalar.transform(X_test)\n",
    "\n",
    "y_train = dataset[:, 0]\n",
    "y_test = testset[:, 0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=len(X_train)//25\n",
    "nnr = MyNeuralNetwork(5, [784, 256, 128, 64, 10], 'relu', 0.1, 'normal',batchsize, 100)\n",
    "\n",
    "\n",
    "nnr.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "accur=nnr.score(X_test,y_test)\n",
    "\n",
    "print(\"Accuracy with Relu :\")\n",
    "print(accur)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([x for x in range(1,len(nnr.train_loss_history) + 1, 1)],nnr.train_loss_history, label = \"Average Training  Loss \" )\n",
    "plt.plot([x for x in range(1,len(nnr.test_loss_history) + 1, 1)],nnr.test_loss_history, label = \"Average Validation  Loss \" )\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'relu_model.sav'\n",
    "pickle.dump(nnr, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "batchsize=len(X_train)//25\n",
    "nnt = MyNeuralNetwork(5, [784, 256, 128, 64, 10], 'tanh', 0.1, 'normal',batchsize, 100)\n",
    "\n",
    "\n",
    "nnt.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "accut=nnt.score(X_test,y_test)\n",
    "\n",
    "print(\"Accuracy with tanh :\")\n",
    "print(accut)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([x for x in range(1,len(nnt.train_loss_history) + 1, 1)],nnt.train_loss_history, label = \"Average Training  Loss \" )\n",
    "plt.plot([x for x in range(1,len(nnt.test_loss_history) + 1, 1)],nnt.test_loss_history, label = \"Average Validation  Loss \" )\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tanh_model.sav'\n",
    "pickle.dump(nnt, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=len(X_train)//25\n",
    "nnl = MyNeuralNetwork(5, [784, 256, 128, 64, 10], 'linear', 0.1, 'normal',batchsize, 100)\n",
    "\n",
    "\n",
    "nnl.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "accul=nnl.score(X_test,y_test)\n",
    "\n",
    "print(\"Accuracy with linear :\")\n",
    "print(accul)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([x for x in range(1,len(nnl.train_loss_history) + 1, 1)],nnl.train_loss_history, label = \"Average Training  Loss \" )\n",
    "plt.plot([x for x in range(1,len(nnl.test_loss_history) + 1, 1)],nnl.test_loss_history, label = \"Average Validation  Loss \" )\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = 'linear_model.sav'\n",
    "pickle.dump(nnl, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=len(X_train)//25\n",
    "nns = MyNeuralNetwork(5, [784, 256, 128, 64, 10], 'sigmoid', 0.1, 'normal',batchsize, 100)\n",
    "\n",
    "\n",
    "nns.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "accus=nns.score(X_test,y_test)\n",
    "\n",
    "print(\"Accuracy with sigmoid :\")\n",
    "print(accus)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([x for x in range(1,len(nns.train_loss_history) + 1, 1)],nns.train_loss_history, label = \"Average Training  Loss \" )\n",
    "plt.plot([x for x in range(1,len(nns.test_loss_history) + 1, 1)],nns.test_loss_history, label = \"Average Validation  Loss \" )\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = 'sigmoid_model.sav'\n",
    "pickle.dump(nns, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmod='tanh_model.sav'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9606"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = pickle.load(open(bestmod, 'rb'))\n",
    "res = best_model.score(X_test, y_test)\n",
    "print(\"Accuracy\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alast,activation,preactivation = best_model.forward(X_train,best_model.weights,best_model.biases)\n",
    "print(activation.keys())\n",
    "final_layer=activation['3']\n",
    "len(final_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30.0, n_iter=1000, verbose=1).fit_transform(final_layer)\n",
    "print(final_layer.shape,tsne[:,0].shape,tsne[:,1].shape)\n",
    "\n",
    "plt.figure(figsize = (11,11))\n",
    "plt.scatter(tsne[:,0],tsne[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MLPClassifier( solver='sgd', hidden_layer_sizes=(256,128,64), activation = 'relu',learning_rate='constant',learning_rate_init=0.1,max_iter=100)\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Relu Activation Score\")\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activation Score\n",
      "0.974\n"
     ]
    }
   ],
   "source": [
    "clf=MLPClassifier( solver='sgd', hidden_layer_sizes=(256,128,64), activation = 'logistic',learning_rate='constant',learning_rate_init=0.1,max_iter=100)\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Sigmoid Activation Score\")\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manish/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:153: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/home/manish/.local/lib/python3.8/site-packages/sklearn/neural_network/_base.py:92: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Activation Score\n",
      "0.098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manish/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf=MLPClassifier( solver='sgd', hidden_layer_sizes=(256,128,64), activation = 'identity',learning_rate='constant',learning_rate_init=0.1,max_iter=100)\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Linear Activation Score\")\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tanh Activation Score\n",
      "0.9732\n"
     ]
    }
   ],
   "source": [
    "clf=MLPClassifier( solver='sgd', hidden_layer_sizes=(256,128,64), activation = 'tanh',learning_rate='constant',learning_rate_init=0.1,max_iter=100)\n",
    "clf.fit(X_train,y_train)\n",
    "print(\" Tanh Activation Score\")\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
